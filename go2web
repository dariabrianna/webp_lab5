#!/usr/bin/env python3
import sys
import socket
import argparse
import re
import os
import json
import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlencode, quote_plus

# Cache management
class Cache:
    def __init__(self, cache_dir=".go2web_cache"):
        self.cache_dir = cache_dir
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)

        def get_cache_path(self, url, content_type=None):
            # make a safe base filename from the URL
            filename = url.replace("://", "_").replace("/", "_").replace("?", "_").replace("&", "_")
            if content_type:
                # sanitize content_type (e.g. "text/html" → "text_html")
                safe_ct = content_type.replace("/", "_")
                filename += f"_{safe_ct}"
            return os.path.join(self.cache_dir, filename)

    def get(self, url, content_type=None, max_age=3600):
        cache_path = self.get_cache_path(url, content_type)
        if os.path.exists(cache_path):
            modified_time = os.path.getmtime(cache_path)
            if (datetime.datetime.now().timestamp() - modified_time) < max_age:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                return cache_data.get('response'), cache_data.get('headers')
        return None, None

    def set(self, url, response, headers, content_type=None):
        cache_path = self.get_cache_path(url, content_type)
        print(f"[cache] writing to {cache_path!r}")
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump({'response': response, 'headers': headers}, f)

# HTTP client implementation

def make_http_request(url, method="GET", headers=None, data=None,
                      follow_redirects=True, accept=None, max_redirects=5):
    cache = Cache()
    parsed_url = urlparse(url)
    hostname = parsed_url.netloc
    path = parsed_url.path or "/"
    if parsed_url.query:
        path += "?" + parsed_url.query
    port = parsed_url.port or (80 if parsed_url.scheme == "http" else 443)

    # Default headers
    headers = headers or {}
    headers.update({
        "Host": hostname.split(":")[0],
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        "Connection": "close"
    })
    if accept:
        headers["Accept"] = accept

    # Pre-request cache check
    if method == "GET":
        cached, cached_headers = (cache.get(url, accept) if accept else cache.get(url))
        if cached:
            print("Using cached response")
            return cached, cached_headers

    # Build HTTP request
    request = f"{method} {path} HTTP/1.1\r\n"
    for k, v in headers.items():
        request += f"{k}: {v}\r\n"
    request += "\r\n"
    if data:
        request += data

    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # HTTPS support
        if parsed_url.scheme == "https":
            import ssl
            context = ssl.create_default_context()
            s = context.wrap_socket(s, server_hostname=hostname.split(":")[0])
        s.connect((hostname.split(":")[0], port))
        s.sendall(request.encode())
        response = b""
        while True:
            chunk = s.recv(4096)
            if not chunk:
                break
            response += chunk
        s.close()
    except Exception as e:
        return f"Error making HTTP request: {e}", {}

    # Parse response
    try:
        header_end = response.find(b"\r\n\r\n")
        raw_headers = response[:header_end].decode('utf-8', errors='ignore')
        body = response[header_end+4:]
        status_line = raw_headers.split("\r\n")[0]
        status_code = int(status_line.split()[1])
        response_headers = {}
        for line in raw_headers.split("\r\n")[1:]:
            if ":" in line:
                key, val = line.split(":", 1)
                response_headers[key.strip()] = val.strip()

        # Redirect handling
        if follow_redirects and status_code in (301,302,303,307,308) and "Location" in response_headers and max_redirects>0:
            loc = response_headers["Location"]
            if not loc.startswith(("http://","https://")):
                if loc.startswith("/"):
                    loc = f"{parsed_url.scheme}://{hostname}{loc}"
                else:
                    loc = f"{parsed_url.scheme}://{hostname}/{loc}"
            print(f"Redirecting to: {loc}")
            return make_http_request(loc, method, headers, data, follow_redirects, accept, max_redirects-1)

        # Decode body
        content_type = response_headers.get("Content-Type", "")
        charset = "utf-8"
        if "charset=" in content_type:
            charset = content_type.split("charset=")[1].split(";")[0].strip()
        if "Content-Encoding" in response_headers:
            enc = response_headers["Content-Encoding"].lower()
            if enc == "gzip":
                import gzip
                body = gzip.decompress(body)
            elif enc == "deflate":
                import zlib
                body = zlib.decompress(body)

        try:
            decoded_body = body.decode(charset, errors='replace')
        except (UnicodeDecodeError, LookupError):
            decoded_body = body.decode('utf-8', errors='replace')

        # Cache write (always for GET 200)
        if method == "GET" and status_code == 200:
            if accept:
                cache.set(url, decoded_body, response_headers, accept)
            else:
                cache.set(url, decoded_body, response_headers)

        return decoded_body, response_headers

    except Exception as e:
        return f"Error parsing response: {e}", {}

# Search and formatting functions

def search(term, search_engine="duckduckgo"):
    if search_engine == "duckduckgo":
        url = f"https://html.duckduckgo.com/html/?q={quote_plus(term)}"
        print(f"Searching for: {url}")
        resp, hdrs = make_http_request(url)
        soup = BeautifulSoup(resp, 'html.parser')
        results = []
        for r in soup.find_all('div', class_='result__body'):
            a = r.find('a', class_='result__a')
            if a and 'href' in a.attrs:
                link = a['href']
                if link.startswith("//duckduckgo.com/l/?uddg="):
                    real = link.split("uddg=")[1].split("&")[0]
                    real = real.replace("%3A",":").replace("%2F","/")
                    results.append((a.get_text(), real))
                    if len(results)>=10:
                        break
        return results
    return []

def format_html_content(content):
    soup = BeautifulSoup(content, 'html.parser')
    title = soup.title.string if soup.title else "No title"
    print(f"\n=== {title} ===\n")
    texts = [t for t in soup.stripped_strings]
    out = re.sub(r'\n{3,}', '\n\n', "\n".join(texts))
    print(out)
    print("\n=== Links ===\n")
    links=[]
    for i,a in enumerate(soup.find_all('a', href=True)):
        print(f"{i+1}. {a.get_text()}: {a['href']}")
        links.append((a.get_text(), a['href']))
    return links

def format_json_content(content):
    try:
        data = json.loads(content)
        return json.dumps(data, indent=2)
    except:
        return content

# Main entry

def main():
    parser = argparse.ArgumentParser(description="Simple web client for HTTP requests")
    parser.add_argument("-u","--url", help="Make an HTTP request to the specified URL")
    parser.add_argument("-s","--search", help="Search term to look up")
    parser.add_argument("--link", type=int, help="Access a specific link number from search results")
    parser.add_argument("--json", action="store_true", help="Request JSON content")
    parser.add_argument("--html", action="store_true", help="Request HTML content")
    args = parser.parse_args()

    last_results = os.path.join(os.path.expanduser("~"), ".go2web_last_results")

    if args.url:
        accept = 'application/json' if args.json else 'text/html' if args.html else None
        resp, hdrs = make_http_request(args.url, accept=accept)
        ctype = hdrs.get('Content-Type','')
        if 'application/json' in ctype:
            print(format_json_content(resp))
        else:
            links = format_html_content(resp)
            with open(last_results,'w') as f:
                json.dump(links, f)

    elif args.search:
        res = search(args.search)
        print(f"\n=== Search Results for '{args.search}' ===\n")
        for i,(t,u) in enumerate(res):
            print(f"{i+1}. {t}\n   {u}\n")
        with open(last_results,'w') as f:
            json.dump(res, f)
        print("\nYou can access any of these links using: go2web --link <number>")

    elif args.link is not None:
        if not os.path.exists(last_results):
            print("No previous search results found. Perform a search first.")
            return
        with open(last_results,'r') as f:
            res = json.load(f)
        if 1 <= args.link <= len(res):
            title, url = res[args.link-1]
            print(f"Accessing: {title} - {url}")
            accept = 'application/json' if args.json else 'text/html' if args.html else None
            resp, hdrs = make_http_request(url, accept=accept)
            ctype = hdrs.get('Content-Type','')
            if 'application/json' in ctype:
                print(format_json_content(resp))
            else:
                format_html_content(resp)
        else:
            print(f"Invalid link number. Choose 1–{len(res)}.")
    else:
        parser.print_help()

if __name__ == '__main__':
    main()
